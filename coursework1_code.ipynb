{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to ML - Decision Tree Coursework COMP70050\n",
    "\n",
    "## Group Members:  \n",
    "* Xuhan Zhang\n",
    "* Judy Zhu\n",
    "* Shijin Zhang"
   ],
   "id": "c3105ea3e26616ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "In this assignment, we will implement a decision tree algorithm and use it to determine one of the indoor\n",
    "locations based on WIFI signal strengths collected from a mobile phone."
   ],
   "id": "3583d72ec833664"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-17T21:36:43.973720Z",
     "start_time": "2024-10-17T21:36:43.970937Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "698f553828941c8d"
  },
  {
   "cell_type": "code",
   "source": [
    "def load_datasets():\n",
    "    # Load the datasets\n",
    "    clean_data_path = 'wifi_db/clean_dataset.txt'\n",
    "    noisy_data_path = 'wifi_db/noisy_dataset.txt'\n",
    "\n",
    "    try:\n",
    "        clean_dataset = np.loadtxt(clean_data_path)\n",
    "        noisy_dataset = np.loadtxt(noisy_data_path)\n",
    "\n",
    "        print(\"Clean dataset loaded successfully with shape:\", clean_dataset.shape)\n",
    "        print(\"Noisy dataset loaded successfully with shape:\", noisy_dataset.shape)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading datasets: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    return clean_dataset, noisy_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T21:36:44.014268Z",
     "start_time": "2024-10-17T21:36:44.010269Z"
    }
   },
   "id": "c76f3a8a6fc505de",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T21:36:44.037144Z",
     "start_time": "2024-10-17T21:36:44.025963Z"
    }
   },
   "cell_type": "code",
   "source": "clean_dataset, noisy_dataset = load_datasets()",
   "id": "c64af16f993d70d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean dataset loaded successfully with shape: (2000, 8)\n",
      "Noisy dataset loaded successfully with shape: (2000, 8)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Creating Decision Trees\n",
    "To create the decision tree, it will write a recursive function called `decision tree learning()`, that takes as\n",
    "arguments a matrix containing the dataset and a depth variable (which is used to compute the maximal depth\n",
    "of the tree, for plotting purposes for instance). The label of the training dataset is assumed to be the last\n",
    "column of the matrix."
   ],
   "id": "c67154cd939848b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T21:36:44.070681Z",
     "start_time": "2024-10-17T21:36:44.060929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self, attribute=None, value=None, left=None, right=None, leaf=False, label=None):\n",
    "        \"\"\"\n",
    "        Initialize a decision tree node.\n",
    "\n",
    "        Args:\n",
    "            attribute (int): The index of the attribute used for splitting.\n",
    "            value (float): The value used for splitting the dataset.\n",
    "            left (DecisionTreeNode): The left child of the node.\n",
    "            right (DecisionTreeNode): The right child of the node.\n",
    "            leaf (bool): Whether this node is a leaf node.\n",
    "            label (int/float): The label of the node if it is a leaf.\n",
    "        \"\"\"\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.leaf = leaf\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def find_split(dataset):\n",
    "    \"\"\"\n",
    "    Find the best split point for the dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset (np.ndarray): Dataset with features and labels.\n",
    "     \n",
    "    Returns: \n",
    "        best_attribute: The best attribute to split on\n",
    "        best_value: The best value to split the best attribute on\n",
    "    \"\"\"\n",
    "    best_attribute = None\n",
    "    best_value = None\n",
    "    best_gain = -1\n",
    "    n_samples, n_features = dataset.shape\n",
    "\n",
    "    # Try to find the best split by iterating all unique values over all the attributes \n",
    "    for attribute in range(n_features - 1):\n",
    "        values = np.unique(dataset[:, attribute])\n",
    "        for value in values:\n",
    "            left_split = dataset[dataset[:, attribute] <= value]\n",
    "            right_split = dataset[dataset[:, attribute] > value]\n",
    "            if len(left_split) > 0 and len(right_split) > 0:\n",
    "                gain = information_gain(dataset, left_split, right_split)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_attribute = attribute\n",
    "                    best_value = value\n",
    "\n",
    "    return best_attribute, best_value\n",
    "\n",
    "\n",
    "def information_gain(parent, left, right):\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a split\n",
    "    \n",
    "    Args:\n",
    "        parent (np.ndarray): The original dataset before the split.\n",
    "        left (np.ndarray): The left split dataset.\n",
    "        right (np.ndarray): The right split dataset.\n",
    "    \n",
    "    Returns:\n",
    "        float: The information gain from the split.\n",
    "    \"\"\"\n",
    "    def entropy(data):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of the given dataset\n",
    "        \"\"\"\n",
    "        labels, counts = np.unique(data[:, -1], return_counts=True)\n",
    "        probabilities = counts / len(data)\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "    n = len(parent)\n",
    "    n_left, n_right = len(left), len(right)\n",
    "    h_parent = entropy(parent)\n",
    "    h_left = entropy(left)\n",
    "    h_right = entropy(right)\n",
    "    remainder = (n_left / n) * h_left + (n_right / n) * h_right\n",
    "    return h_parent - remainder\n",
    "\n",
    "\n",
    "def decision_tree_learning(dataset, depth=0):\n",
    "    \"\"\"\n",
    "    Train a decision tree recursively.\n",
    "\n",
    "    Args:\n",
    "        dataset (np.ndarray): The dataset to be used for training.\n",
    "        depth (int): The current depth of the tree.\n",
    "\n",
    "    Returns:\n",
    "        DecisionTreeNode: The root node of the trained decision tree.\n",
    "    \"\"\"\n",
    "    labels = dataset[:, -1]\n",
    "    # If all labels are the same, return a leaf node\n",
    "    if len(np.unique(labels)) == 1:\n",
    "        return DecisionTreeNode(leaf=True, label=labels[0])\n",
    "    \n",
    "    best_attribute, best_value = find_split(dataset)\n",
    "    if best_attribute is None:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        majority_label = unique[np.argmax(counts)]\n",
    "        return DecisionTreeNode(leaf=True, label=majority_label)\n",
    "    \n",
    "    # Split the dataset and grow the tree recursively \n",
    "    left_split = dataset[dataset[:, best_attribute] <= best_value]\n",
    "    right_split = dataset[dataset[:, best_attribute] > best_value]\n",
    "    left_branch = decision_tree_learning(left_split, depth + 1)\n",
    "    right_branch = decision_tree_learning(right_split, depth + 1)\n",
    "    \n",
    "    return DecisionTreeNode(attribute=best_attribute, value=best_value, left=left_branch, right=right_branch)"
   ],
   "id": "f2b304f2abda80e0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T21:36:44.411875Z",
     "start_time": "2024-10-17T21:36:44.099627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training decision tree on clean dataset...\")\n",
    "decision_tree = decision_tree_learning(clean_dataset)\n",
    "print(\"Decision tree trained successfully.\")"
   ],
   "id": "a78bbee3195c7807",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training decision tree on clean dataset...\n",
      "Decision tree trained successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "intro2ml",
   "language": "python",
   "display_name": "Python (intro2ml)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
